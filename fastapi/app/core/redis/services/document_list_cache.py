"""
æ–‡æ¡£åˆ—è¡¨ç¼“å­˜æœåŠ¡
åŠŸèƒ½ï¼šä¸“é—¨å¤„ç†æ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢çš„ç¼“å­˜é€»è¾‘
"""
import json
import time
import hashlib
from typing import Dict, Any, Optional, Union
from sqlalchemy.orm import Session

from ..client import RedisClient


class DocumentListCacheService:
    """æ–‡æ¡£åˆ—è¡¨ç¼“å­˜æœåŠ¡"""

    def __init__(self):
        self.redis_client = RedisClient()

        # ç¼“å­˜é…ç½®
        self.public_list_ttl = 600  # æŠ€æœ¯å¹¿åœºåˆ—è¡¨ï¼š10åˆ†é’Ÿ
        self.user_list_ttl = 1200  # ä¸ªäººæ–‡æ¡£åˆ—è¡¨ï¼š20åˆ†é’Ÿ
        self.key_prefix = "doc_list"

        print(f"ğŸ“„ [DOC_LIST_CACHE] æ–‡æ¡£åˆ—è¡¨ç¼“å­˜æœåŠ¡åˆå§‹åŒ–")
        print(f"ğŸ“„ [DOC_LIST_CACHE] å…¬å¼€åˆ—è¡¨TTL: {self.public_list_ttl}ç§’")
        print(f"ğŸ“„ [DOC_LIST_CACHE] ç”¨æˆ·åˆ—è¡¨TTL: {self.user_list_ttl}ç§’")
        print(f"ğŸ“„ [DOC_LIST_CACHE] Rediså¯ç”¨: {self.redis_client.is_available()}")

    def _generate_search_hash(self, search_text: Optional[str]) -> str:
        """ç”Ÿæˆæœç´¢å…³é”®è¯çš„å“ˆå¸Œå€¼ï¼ˆé¿å…Keyè¿‡é•¿ï¼‰"""
        if not search_text:
            return "none"

        # ä½¿ç”¨MD5ç”ŸæˆçŸ­å“ˆå¸Œ
        hash_obj = hashlib.md5(search_text.encode('utf-8'))
        hash_value = hash_obj.hexdigest()[:8]  # å–å‰8ä½

        print(f"ğŸ” [DOC_LIST_CACHE] æœç´¢è¯å“ˆå¸Œ: '{search_text}' -> {hash_value}")
        return hash_value

    def _build_public_cache_key(
            self,
            page: int,
            size: int,
            search: Optional[str] = None,
            file_type: Optional[str] = None,
            time_filter: Optional[str] = None,
            sort_by: str = "latest"
    ) -> str:
        """æ„å»ºæŠ€æœ¯å¹¿åœºæ–‡æ¡£åˆ—è¡¨ç¼“å­˜Key"""

        # å¤„ç†å¯é€‰å‚æ•°
        search_hash = self._generate_search_hash(search)
        file_type_str = file_type or "none"
        time_filter_str = time_filter or "none"

        # æ„å»ºç¼“å­˜Key
        key = f"{self.key_prefix}:public:p{page}:s{size}:q{search_hash}:t{file_type_str}:time{time_filter_str}:sort{sort_by}"

        print(f"ğŸ”‘ [DOC_LIST_CACHE] æ„å»ºå…¬å¼€åˆ—è¡¨ç¼“å­˜Key: {key}")
        print(
            f"ğŸ”‘ [DOC_LIST_CACHE] å‚æ•°è¯¦æƒ…: page={page}, size={size}, search='{search}', type={file_type_str}, time={time_filter_str}, sort={sort_by}")

        return key

    def _build_user_cache_key(
            self,
            user_id: int,
            page: int,
            size: int,
            folder_id: Optional[int] = None
    ) -> str:
        """æ„å»ºä¸ªäººæ–‡æ¡£åˆ—è¡¨ç¼“å­˜Key"""

        folder_str = str(folder_id) if folder_id is not None else "none"
        key = f"{self.key_prefix}:user{user_id}:p{page}:s{size}:f{folder_str}"

        print(f"ğŸ”‘ [DOC_LIST_CACHE] æ„å»ºç”¨æˆ·åˆ—è¡¨ç¼“å­˜Key: {key}")
        print(f"ğŸ”‘ [DOC_LIST_CACHE] å‚æ•°è¯¦æƒ…: user_id={user_id}, page={page}, size={size}, folder_id={folder_id}")

        return key

    async def get_public_document_list(
            self,
            db: Session,
            query_func,  # ä¼ å…¥æŸ¥è¯¢å‡½æ•°
            page: int,
            size: int,
            search: Optional[str] = None,
            file_type: Optional[str] = None,
            time_filter: Optional[str] = None,
            sort_by: str = "latest",
            **kwargs
    ) -> Dict[str, Any]:
        """
        è·å–æŠ€æœ¯å¹¿åœºæ–‡æ¡£åˆ—è¡¨ï¼ˆç¼“å­˜ä¼˜åŒ–ç‰ˆï¼‰

        Args:
            db: æ•°æ®åº“ä¼šè¯
            query_func: å®é™…çš„æŸ¥è¯¢å‡½æ•°
            page: é¡µç 
            size: æ¯é¡µæ•°é‡
            search: æœç´¢å…³é”®è¯
            file_type: æ–‡ä»¶ç±»å‹ç­›é€‰
            time_filter: æ—¶é—´ç­›é€‰
            sort_by: æ’åºæ–¹å¼
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™æŸ¥è¯¢å‡½æ•°
        """
        cache_key = self._build_public_cache_key(page, size, search, file_type, time_filter, sort_by)

        print(f"ğŸ“„ [DOC_LIST_CACHE] å¼€å§‹è·å–æŠ€æœ¯å¹¿åœºæ–‡æ¡£åˆ—è¡¨ç¼“å­˜...")
        print(f"ğŸ“„ [DOC_LIST_CACHE] ç¼“å­˜Key: {cache_key}")

        # ğŸ” ç¬¬ä¸€æ­¥ï¼šå°è¯•ä»ç¼“å­˜è·å–
        cached_data = await self._get_from_cache(cache_key)
        if cached_data:
            print(f"âœ… [DOC_LIST_CACHE] ç¼“å­˜å‘½ä¸­! è¿”å›ç¼“å­˜æ•°æ®")

            # æ·»åŠ ç¼“å­˜ä¿¡æ¯
            cached_data["cache_info"] = {
                "cached": True,
                "cache_time": cached_data.get("_cache_time"),
                "ttl_remaining": self.redis_client.ttl(cache_key),
                "cache_type": "public_document_list",
                "cache_key": cache_key
            }

            return cached_data

        # ğŸ—„ï¸ ç¬¬äºŒæ­¥ï¼šç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“
        print(f"âŒ [DOC_LIST_CACHE] ç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“...")
        list_data = await self._query_public_list(db, query_func, page, size, search, file_type, time_filter, sort_by,
                                                  **kwargs)

        # ğŸ’¾ ç¬¬ä¸‰æ­¥ï¼šå†™å…¥ç¼“å­˜
        await self._save_to_cache(cache_key, list_data, self.public_list_ttl)

        # æ·»åŠ ç¼“å­˜ä¿¡æ¯
        list_data["cache_info"] = {
            "cached": False,
            "cache_time": list_data.get("_cache_time"),
            "ttl_remaining": self.public_list_ttl,
            "cache_type": "public_document_list",
            "cache_key": cache_key
        }

        return list_data

    async def get_user_document_list(
            self,
            db: Session,
            query_func,  # ä¼ å…¥æŸ¥è¯¢å‡½æ•°
            user_id: int,
            page: int,
            size: int,
            folder_id: Optional[int] = None,
            **kwargs
    ) -> Dict[str, Any]:
        """
        è·å–ä¸ªäººæ–‡æ¡£åˆ—è¡¨ï¼ˆç¼“å­˜ä¼˜åŒ–ç‰ˆï¼‰

        Args:
            db: æ•°æ®åº“ä¼šè¯
            query_func: å®é™…çš„æŸ¥è¯¢å‡½æ•°
            user_id: ç”¨æˆ·ID
            page: é¡µç 
            size: æ¯é¡µæ•°é‡
            folder_id: æ–‡ä»¶å¤¹IDç­›é€‰
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™æŸ¥è¯¢å‡½æ•°
        """
        cache_key = self._build_user_cache_key(user_id, page, size, folder_id)

        print(f"ğŸ“„ [DOC_LIST_CACHE] å¼€å§‹è·å–ç”¨æˆ·æ–‡æ¡£åˆ—è¡¨ç¼“å­˜...")
        print(f"ğŸ“„ [DOC_LIST_CACHE] ç¼“å­˜Key: {cache_key}")

        # ğŸ” ç¬¬ä¸€æ­¥ï¼šå°è¯•ä»ç¼“å­˜è·å–
        cached_data = await self._get_from_cache(cache_key)
        if cached_data:
            print(f"âœ… [DOC_LIST_CACHE] ç¼“å­˜å‘½ä¸­! è¿”å›ç¼“å­˜æ•°æ®")

            # æ·»åŠ ç¼“å­˜ä¿¡æ¯
            cached_data["cache_info"] = {
                "cached": True,
                "cache_time": cached_data.get("_cache_time"),
                "ttl_remaining": self.redis_client.ttl(cache_key),
                "cache_type": "user_document_list",
                "cache_key": cache_key
            }

            return cached_data

        # ğŸ—„ï¸ ç¬¬äºŒæ­¥ï¼šç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“
        print(f"âŒ [DOC_LIST_CACHE] ç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“...")
        list_data = await self._query_user_list(db, query_func, user_id, page, size, folder_id, **kwargs)

        # ğŸ’¾ ç¬¬ä¸‰æ­¥ï¼šå†™å…¥ç¼“å­˜
        await self._save_to_cache(cache_key, list_data, self.user_list_ttl)

        # æ·»åŠ ç¼“å­˜ä¿¡æ¯
        list_data["cache_info"] = {
            "cached": False,
            "cache_time": list_data.get("_cache_time"),
            "ttl_remaining": self.user_list_ttl,
            "cache_type": "user_document_list",
            "cache_key": cache_key
        }

        return list_data

    async def _query_public_list(self, db: Session, query_func, page: int, size: int, search: Optional[str],
                                 file_type: Optional[str], time_filter: Optional[str], sort_by: str, **kwargs) -> Dict[
        str, Any]:
        """æŸ¥è¯¢æŠ€æœ¯å¹¿åœºæ–‡æ¡£åˆ—è¡¨ï¼ˆå¸¦è¯¦ç»†æ€§èƒ½ç›‘æ§ï¼‰"""
        print(f"ğŸ—„ï¸ [DOC_LIST_CACHE] å¼€å§‹æŠ€æœ¯å¹¿åœºæ–‡æ¡£åˆ—è¡¨æ•°æ®åº“æŸ¥è¯¢...")
        start_time = time.time()

        try:
            # è°ƒç”¨å®é™…çš„æŸ¥è¯¢å‡½æ•°
            result = query_func(
                page=page,
                size=size,
                search=search,
                file_type=file_type,
                time_filter=time_filter,
                sort_by=sort_by,
                **kwargs
            )

            query_time = (time.time() - start_time) * 1000

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆå¦‚æœæ˜¯Pydanticæ¨¡å‹ï¼‰
            if hasattr(result, 'model_dump'):
                result_dict = result.model_dump()
            elif hasattr(result, '__dict__'):
                result_dict = result.__dict__
            else:
                result_dict = result

            # æ·»åŠ æ€§èƒ½ä¿¡æ¯
            result_dict.update({
                "_cache_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "_query_performance": {
                    "query_type": "public_document_list",
                    "total_ms": round(query_time, 2),
                    "page": page,
                    "size": size,
                    "total_documents": result_dict.get("total", 0),
                    "returned_documents": len(result_dict.get("documents", []))
                }
            })

            print(f"âœ… [DOC_LIST_CACHE] æŠ€æœ¯å¹¿åœºåˆ—è¡¨æŸ¥è¯¢å®Œæˆï¼Œæ€»è€—æ—¶: {query_time:.2f}ms")
            print(
                f"ğŸ“Š [DOC_LIST_CACHE] æŸ¥è¯¢ç»“æœ: æ€»æ•°{result_dict.get('total', 0)}, è¿”å›{len(result_dict.get('documents', []))}æ¡")

            return result_dict

        except Exception as e:
            query_time = (time.time() - start_time) * 1000
            print(f"âŒ [DOC_LIST_CACHE] æŠ€æœ¯å¹¿åœºåˆ—è¡¨æŸ¥è¯¢å¤±è´¥ ({query_time:.2f}ms): {e}")
            raise

    async def _query_user_list(self, db: Session, query_func, user_id: int, page: int, size: int,
                               folder_id: Optional[int], **kwargs) -> Dict[str, Any]:
        """æŸ¥è¯¢ä¸ªäººæ–‡æ¡£åˆ—è¡¨ï¼ˆå¸¦è¯¦ç»†æ€§èƒ½ç›‘æ§ï¼‰"""
        print(f"ğŸ—„ï¸ [DOC_LIST_CACHE] å¼€å§‹ç”¨æˆ·æ–‡æ¡£åˆ—è¡¨æ•°æ®åº“æŸ¥è¯¢...")
        start_time = time.time()

        try:
            # è°ƒç”¨å®é™…çš„æŸ¥è¯¢å‡½æ•°
            result = query_func(
                db=db,
                user_id=user_id,
                folder_id=folder_id,
                page=page,
                page_size=size,
                **kwargs
            )

            query_time = (time.time() - start_time) * 1000

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆå¦‚æœæ˜¯Pydanticæ¨¡å‹ï¼‰
            if hasattr(result, 'model_dump'):
                result_dict = result.model_dump()
            elif hasattr(result, '__dict__'):
                result_dict = result.__dict__
            else:
                result_dict = result

            # æ·»åŠ æ€§èƒ½ä¿¡æ¯
            result_dict.update({
                "_cache_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "_query_performance": {
                    "query_type": "user_document_list",
                    "total_ms": round(query_time, 2),
                    "user_id": user_id,
                    "folder_id": folder_id,
                    "page": page,
                    "size": size,
                    "total_documents": result_dict.get("total", 0),
                    "returned_documents": len(result_dict.get("documents", []))
                }
            })

            print(f"âœ… [DOC_LIST_CACHE] ç”¨æˆ·æ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢å®Œæˆï¼Œæ€»è€—æ—¶: {query_time:.2f}ms")
            print(
                f"ğŸ“Š [DOC_LIST_CACHE] æŸ¥è¯¢ç»“æœ: ç”¨æˆ·{user_id}, æ€»æ•°{result_dict.get('total', 0)}, è¿”å›{len(result_dict.get('documents', []))}æ¡")

            return result_dict

        except Exception as e:
            query_time = (time.time() - start_time) * 1000
            print(f"âŒ [DOC_LIST_CACHE] ç”¨æˆ·æ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢å¤±è´¥ ({query_time:.2f}ms): {e}")
            raise

    async def _get_from_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [DOC_LIST_CACHE] Redisä¸å¯ç”¨ï¼Œè·³è¿‡ç¼“å­˜è¯»å–")
            return None

        try:
            start_time = time.time()
            cached_str = self.redis_client.get(cache_key)
            read_time = (time.time() - start_time) * 1000

            if cached_str:
                data = json.loads(cached_str)
                print(f"ğŸ’¾ [DOC_LIST_CACHE] ç¼“å­˜è¯»å–æˆåŠŸ ({read_time:.2f}ms), æ•°æ®å¤§å°: {len(cached_str)} bytes")
                return data
            else:
                print(f"ğŸ’¾ [DOC_LIST_CACHE] ç¼“å­˜Keyä¸å­˜åœ¨ ({read_time:.2f}ms)")
                return None

        except Exception as e:
            print(f"âŒ [DOC_LIST_CACHE] ç¼“å­˜è¯»å–å¤±è´¥: {e}")
            return None

    async def _save_to_cache(self, cache_key: str, data: Dict[str, Any], ttl: int) -> bool:
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [DOC_LIST_CACHE] Redisä¸å¯ç”¨ï¼Œè·³è¿‡ç¼“å­˜å†™å…¥")
            return False

        try:
            start_time = time.time()
            data_str = json.dumps(data, ensure_ascii=False, default=str)  # default=strå¤„ç†datetimeç­‰ç±»å‹
            success = self.redis_client.setex(cache_key, ttl, data_str)
            write_time = (time.time() - start_time) * 1000

            if success:
                print(f"ğŸ’¾ [DOC_LIST_CACHE] ç¼“å­˜å†™å…¥æˆåŠŸ ({write_time:.2f}ms)")
                print(f"ğŸ’¾ [DOC_LIST_CACHE] æ•°æ®å¤§å°: {len(data_str)} bytes, TTL: {ttl}ç§’")
                return True
            else:
                print(f"âš ï¸ [DOC_LIST_CACHE] ç¼“å­˜å†™å…¥å¤±è´¥ ({write_time:.2f}ms)")
                return False

        except Exception as e:
            print(f"âŒ [DOC_LIST_CACHE] ç¼“å­˜å†™å…¥å¼‚å¸¸: {e}")
            return False

    async def invalidate_public_list_cache(self, pattern: str = "doc_list:public:*") -> int:
        """æ¸…é™¤æŠ€æœ¯å¹¿åœºæ–‡æ¡£åˆ—è¡¨ç¼“å­˜ï¼ˆå½“æœ‰æ–°æ–‡æ¡£å‘å¸ƒæ—¶è°ƒç”¨ï¼‰"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [DOC_LIST_CACHE] Redisä¸å¯ç”¨ï¼Œæ— æ³•æ¸…é™¤ç¼“å­˜")
            return 0

        try:
            # è·å–åŒ¹é…çš„Key
            keys = self.redis_client.keys(pattern)
            if not keys:
                print(f"â„¹ï¸ [DOC_LIST_CACHE] æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å…¬å¼€åˆ—è¡¨ç¼“å­˜Key")
                return 0

            # æ‰¹é‡åˆ é™¤
            deleted_count = 0
            for key in keys:
                if self.redis_client.delete(key):
                    deleted_count += 1

            print(f"âœ… [DOC_LIST_CACHE] å·²æ¸…é™¤{deleted_count}ä¸ªæŠ€æœ¯å¹¿åœºåˆ—è¡¨ç¼“å­˜")
            return deleted_count

        except Exception as e:
            print(f"âŒ [DOC_LIST_CACHE] æ¸…é™¤å…¬å¼€åˆ—è¡¨ç¼“å­˜å¤±è´¥: {e}")
            return 0

    async def invalidate_user_list_cache(self, user_id: int) -> int:
        """æ¸…é™¤æŒ‡å®šç”¨æˆ·çš„æ–‡æ¡£åˆ—è¡¨ç¼“å­˜ï¼ˆå½“ç”¨æˆ·æ–‡æ¡£å˜æ›´æ—¶è°ƒç”¨ï¼‰"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [DOC_LIST_CACHE] Redisä¸å¯ç”¨ï¼Œæ— æ³•æ¸…é™¤ç¼“å­˜")
            return 0

        try:
            pattern = f"doc_list:user{user_id}:*"
            keys = self.redis_client.keys(pattern)

            if not keys:
                print(f"â„¹ï¸ [DOC_LIST_CACHE] æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·{user_id}çš„åˆ—è¡¨ç¼“å­˜")
                return 0

            # æ‰¹é‡åˆ é™¤
            deleted_count = 0
            for key in keys:
                if self.redis_client.delete(key):
                    deleted_count += 1

            print(f"âœ… [DOC_LIST_CACHE] å·²æ¸…é™¤ç”¨æˆ·{user_id}çš„{deleted_count}ä¸ªåˆ—è¡¨ç¼“å­˜")
            return deleted_count

        except Exception as e:
            print(f"âŒ [DOC_LIST_CACHE] æ¸…é™¤ç”¨æˆ·åˆ—è¡¨ç¼“å­˜å¤±è´¥: {e}")
            return 0


# å…¨å±€å®ä¾‹
document_list_cache_service = DocumentListCacheService()