"""
ç»Ÿè®¡æ•°æ®ç¼“å­˜æœåŠ¡
åŠŸèƒ½ï¼šä¸“é—¨å¤„ç†ç»Ÿè®¡æ•°æ®çš„ç¼“å­˜é€»è¾‘
"""
import json
import time
from typing import Dict, Any, Optional
from sqlalchemy.orm import Session
from sqlalchemy import func

from ..client import RedisClient
from ....modules.v2.document_manager.models import Document, Folder, DocumentStatus


class StatsCacheService:
    """ç»Ÿè®¡ç¼“å­˜æœåŠ¡"""

    def __init__(self):
        self.redis_client = RedisClient()

        # ç¼“å­˜é…ç½®
        self.ttl = 1800  # 30åˆ†é’Ÿ (ç»Ÿè®¡æ•°æ®å˜åŒ–ä¸é¢‘ç¹)
        self.key_prefix = "stats"

        print(f"ğŸ’¾ [CACHE] ç»Ÿè®¡ç¼“å­˜æœåŠ¡åˆå§‹åŒ–")
        print(f"ğŸ’¾ [CACHE] TTL: {self.ttl}ç§’, Rediså¯ç”¨: {self.redis_client.is_available()}")

    def _build_cache_key(self, cache_type: str, user_id: int) -> str:
        """æ„å»ºç¼“å­˜Key"""
        key = f"{self.key_prefix}:{cache_type}:{user_id}"
        print(f"ğŸ”‘ [CACHE] æ„å»ºç¼“å­˜Key: {key}")
        return key

    async def get_user_document_stats(self, db: Session, user_id: int) -> Dict[str, Any]:
        """
        è·å–ç”¨æˆ·æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯ï¼ˆç¼“å­˜ä¼˜åŒ–ç‰ˆï¼‰
        """
        cache_key = self._build_cache_key("user_docs", user_id)

        print(f"ğŸ’¾ [CACHE] å¼€å§‹è·å–ç”¨æˆ·ç»Ÿè®¡ç¼“å­˜...")
        print(f"ğŸ’¾ [CACHE] ç”¨æˆ·ID: {user_id}, ç¼“å­˜Key: {cache_key}")

        # ğŸ” ç¬¬ä¸€æ­¥ï¼šå°è¯•ä»ç¼“å­˜è·å–
        cached_data = await self._get_from_cache(cache_key)
        if cached_data:
            print(f"âœ… [CACHE] ç¼“å­˜å‘½ä¸­! è¿”å›ç¼“å­˜æ•°æ®")

            # æ·»åŠ ç¼“å­˜ä¿¡æ¯
            cached_data["cache_info"] = {
                "cached": True,
                "cache_time": cached_data.get("_cache_time"),
                "ttl_remaining": self.redis_client.ttl(cache_key)
            }

            return cached_data

        # ğŸ—„ï¸ ç¬¬äºŒæ­¥ï¼šç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“
        print(f"âŒ [CACHE] ç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“...")
        stats_data = await self._query_database_stats(db, user_id)

        # ğŸ’¾ ç¬¬ä¸‰æ­¥ï¼šå†™å…¥ç¼“å­˜
        await self._save_to_cache(cache_key, stats_data)

        # æ·»åŠ ç¼“å­˜ä¿¡æ¯
        stats_data["cache_info"] = {
            "cached": False,
            "cache_time": stats_data.get("_cache_time"),
            "ttl_remaining": self.ttl
        }

        return stats_data

    async def _query_database_stats(self, db: Session, user_id: int) -> Dict[str, Any]:
        """æŸ¥è¯¢æ•°æ®åº“ç»Ÿè®¡æ•°æ®ï¼ˆå¸¦æ€§èƒ½ç›‘æ§ï¼‰"""
        print(f"ğŸ—„ï¸ [CACHE] å¼€å§‹æ•°æ®åº“æŸ¥è¯¢...")
        start_time = time.time()

        try:
            # æŸ¥è¯¢1ï¼šæ€»æ–‡æ¡£æ•°
            query1_start = time.time()
            total_docs = db.query(Document).filter(Document.user_id == user_id).count()
            query1_time = (time.time() - query1_start) * 1000
            print(f"ğŸ—„ï¸ [CACHE] æŸ¥è¯¢1å®Œæˆ: æ€»æ–‡æ¡£æ•° = {total_docs} ({query1_time:.2f}ms)")

            # æŸ¥è¯¢2ï¼šæŒ‰çŠ¶æ€ç»Ÿè®¡
            query2_start = time.time()
            status_stats = db.query(
                Document.status,
                func.count(Document.id)
            ).filter(
                Document.user_id == user_id
            ).group_by(Document.status).all()
            query2_time = (time.time() - query2_start) * 1000
            print(f"ğŸ—„ï¸ [CACHE] æŸ¥è¯¢2å®Œæˆ: çŠ¶æ€ç»Ÿè®¡ = {len(status_stats)}ç§çŠ¶æ€ ({query2_time:.2f}ms)")

            # æŸ¥è¯¢3ï¼šæ–‡ä»¶å¤¹æ•°é‡
            query3_start = time.time()
            total_folders = db.query(Folder).filter(Folder.user_id == user_id).count()
            query3_time = (time.time() - query3_start) * 1000
            print(f"ğŸ—„ï¸ [CACHE] æŸ¥è¯¢3å®Œæˆ: æ–‡ä»¶å¤¹æ•° = {total_folders} ({query3_time:.2f}ms)")

            # æ ¼å¼åŒ–çŠ¶æ€ç»Ÿè®¡
            status_dict = {status.value: 0 for status in DocumentStatus}
            for status, count in status_stats:
                status_dict[status] = count

            # æ„å»ºç»“æœ
            result = {
                "total_documents": total_docs,
                "total_folders": total_folders,
                "documents_by_status": status_dict,
                "user_id": user_id,
                "_cache_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "_query_performance": {
                    "total_docs_ms": round(query1_time, 2),
                    "status_stats_ms": round(query2_time, 2),
                    "total_folders_ms": round(query3_time, 2),
                    "total_ms": round((time.time() - start_time) * 1000, 2)
                }
            }

            total_time = (time.time() - start_time) * 1000
            print(f"âœ… [CACHE] æ•°æ®åº“æŸ¥è¯¢å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ms")

            return result

        except Exception as e:
            query_time = (time.time() - start_time) * 1000
            print(f"âŒ [CACHE] æ•°æ®åº“æŸ¥è¯¢å¤±è´¥ ({query_time:.2f}ms): {e}")
            raise

    async def _get_from_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [CACHE] Redisä¸å¯ç”¨ï¼Œè·³è¿‡ç¼“å­˜è¯»å–")
            return None

        try:
            start_time = time.time()
            cached_str = self.redis_client.get(cache_key)
            read_time = (time.time() - start_time) * 1000

            if cached_str:
                data = json.loads(cached_str)
                print(f"ğŸ’¾ [CACHE] ç¼“å­˜è¯»å–æˆåŠŸ ({read_time:.2f}ms), æ•°æ®å¤§å°: {len(cached_str)} bytes")
                return data
            else:
                print(f"ğŸ’¾ [CACHE] ç¼“å­˜Keyä¸å­˜åœ¨ ({read_time:.2f}ms)")
                return None

        except Exception as e:
            print(f"âŒ [CACHE] ç¼“å­˜è¯»å–å¤±è´¥: {e}")
            return None

    async def _save_to_cache(self, cache_key: str, data: Dict[str, Any]) -> bool:
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [CACHE] Redisä¸å¯ç”¨ï¼Œè·³è¿‡ç¼“å­˜å†™å…¥")
            return False

        try:
            start_time = time.time()
            data_str = json.dumps(data, ensure_ascii=False)
            success = self.redis_client.setex(cache_key, self.ttl, data_str)
            write_time = (time.time() - start_time) * 1000

            if success:
                print(f"ğŸ’¾ [CACHE] ç¼“å­˜å†™å…¥æˆåŠŸ ({write_time:.2f}ms)")
                print(f"ğŸ’¾ [CACHE] æ•°æ®å¤§å°: {len(data_str)} bytes, TTL: {self.ttl}ç§’")
                return True
            else:
                print(f"âš ï¸ [CACHE] ç¼“å­˜å†™å…¥å¤±è´¥ ({write_time:.2f}ms)")
                return False

        except Exception as e:
            print(f"âŒ [CACHE] ç¼“å­˜å†™å…¥å¼‚å¸¸: {e}")
            return False

    async def invalidate_user_stats(self, user_id: int) -> bool:
        """æ¸…é™¤ç”¨æˆ·ç»Ÿè®¡ç¼“å­˜ï¼ˆå½“æ•°æ®å˜æ›´æ—¶è°ƒç”¨ï¼‰"""
        cache_key = self._build_cache_key("user_docs", user_id)

        if not self.redis_client.is_available():
            print(f"âš ï¸ [CACHE] Redisä¸å¯ç”¨ï¼Œæ— æ³•æ¸…é™¤ç¼“å­˜")
            return False

        try:
            result = self.redis_client.delete(cache_key)
            if result:
                print(f"âœ… [CACHE] ç”¨æˆ·ç»Ÿè®¡ç¼“å­˜å·²æ¸…é™¤: {cache_key}")
            else:
                print(f"â„¹ï¸ [CACHE] ç¼“å­˜Keyä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…é™¤: {cache_key}")
            return bool(result)

        except Exception as e:
            print(f"âŒ [CACHE] æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")
            return False


# å…¨å±€å®ä¾‹
stats_cache_service = StatsCacheService()