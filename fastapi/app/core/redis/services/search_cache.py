"""
æœç´¢ç»“æœç¼“å­˜æœåŠ¡
åŠŸèƒ½ï¼šä¸“é—¨å¤„ç†æœç´¢ç»“æœçš„ç¼“å­˜é€»è¾‘
"""
import json
import time
import hashlib
from typing import Dict, Any, Optional, Callable
from sqlalchemy.orm import Session

from ..client import RedisClient


class SearchCacheService:
    """æœç´¢ç»“æœç¼“å­˜æœåŠ¡"""

    def __init__(self):
        self.redis_client = RedisClient()

        # ç¼“å­˜é…ç½®
        self.search_ttl = 480  # æœç´¢ç»“æœç¼“å­˜8åˆ†é’Ÿ
        self.key_prefix = "search_cache"

        print(f"ğŸ” [CACHE] æœç´¢ç¼“å­˜æœåŠ¡åˆå§‹åŒ–")
        print(f"ğŸ” [CACHE] æœç´¢ç»“æœTTL: {self.search_ttl}ç§’")
        print(f"ğŸ” [CACHE] Rediså¯ç”¨: {self.redis_client.is_available()}")

    def _build_search_cache_key(self, keyword: str, page: int, size: int, file_type: Optional[str] = None) -> str:
        """æ„å»ºæœç´¢ç¼“å­˜Key"""
        # å¯¹æœç´¢å…³é”®è¯è¿›è¡Œå“ˆå¸Œå¤„ç†ï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦å’Œé•¿åº¦é—®é¢˜
        keyword_hash = self._generate_keyword_hash(keyword)
        file_type_str = file_type or "none"

        key = f"{self.key_prefix}:keyword_{keyword_hash}:p{page}:s{size}:t{file_type_str}"
        print(f"ğŸ”‘ [CACHE] æ„å»ºæœç´¢ç¼“å­˜Key: {key}")
        print(f"ğŸ”‘ [CACHE] åŸå§‹å…³é”®è¯: '{keyword}' -> å“ˆå¸Œ: {keyword_hash}")
        return key

    def _generate_keyword_hash(self, keyword: str) -> str:
        """ç”Ÿæˆå…³é”®è¯å“ˆå¸Œ"""
        # ä½¿ç”¨MD5å“ˆå¸Œï¼Œå–å‰8ä½ä½œä¸ºæ ‡è¯†
        hash_obj = hashlib.md5(keyword.lower().strip().encode('utf-8'))
        keyword_hash = hash_obj.hexdigest()[:8]
        return keyword_hash

    async def get_search_results(
            self,
            db: Session,
            query_func: Callable,
            keyword: str,
            page: int = 1,
            size: int = 20,
            file_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        è·å–æœç´¢ç»“æœï¼ˆç¼“å­˜ä¼˜åŒ–ç‰ˆï¼‰
        """
        cache_key = self._build_search_cache_key(keyword, page, size, file_type)

        print(f"ğŸ” [CACHE] å¼€å§‹è·å–æœç´¢ç»“æœç¼“å­˜...")
        print(f"ğŸ” [CACHE] æœç´¢å‚æ•°: keyword='{keyword}', page={page}, size={size}, file_type={file_type}")
        print(f"ğŸ” [CACHE] ç¼“å­˜Key: {cache_key}")

        # ğŸ” ç¬¬ä¸€æ­¥ï¼šå°è¯•ä»ç¼“å­˜è·å–
        cached_data = await self._get_from_cache(cache_key)
        if cached_data:
            print(f"âœ… [CACHE] æœç´¢ç»“æœç¼“å­˜å‘½ä¸­! è¿”å›ç¼“å­˜æ•°æ®")

            # æ·»åŠ ç¼“å­˜ä¿¡æ¯
            cached_data["cache_info"] = {
                "cached": True,
                "cache_time": cached_data.get("_cache_time"),
                "ttl_remaining": self.redis_client.ttl(cache_key),
                "search_keyword": keyword,
                "keyword_hash": self._generate_keyword_hash(keyword)
            }

            return cached_data

        # ğŸ—„ï¸ ç¬¬äºŒæ­¥ï¼šç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“
        print(f"âŒ [CACHE] æœç´¢ç»“æœç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“...")
        search_data = await self._query_search_results(db, query_func, keyword, page, size, file_type)

        # ğŸ’¾ ç¬¬ä¸‰æ­¥ï¼šå†™å…¥ç¼“å­˜
        await self._save_to_cache(cache_key, search_data)

        # æ·»åŠ ç¼“å­˜ä¿¡æ¯
        search_data["cache_info"] = {
            "cached": False,
            "cache_time": search_data.get("_cache_time"),
            "ttl_remaining": self.search_ttl,
            "search_keyword": keyword,
            "keyword_hash": self._generate_keyword_hash(keyword)
        }

        return search_data

    async def _query_search_results(
            self,
            db: Session,
            query_func: Callable,
            keyword: str,
            page: int,
            size: int,
            file_type: Optional[str]
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢æœç´¢ç»“æœæ•°æ®ï¼ˆå¸¦æ€§èƒ½ç›‘æ§ï¼‰"""
        print(f"ğŸ—„ï¸ [CACHE] å¼€å§‹æŸ¥è¯¢æœç´¢ç»“æœæ•°æ®åº“...")
        start_time = time.time()

        try:
            # è°ƒç”¨ä¼ å…¥çš„æŸ¥è¯¢å‡½æ•°
            result = query_func(
                keyword=keyword,
                page=page,
                size=size,
                file_type=file_type
            )

            query_time = (time.time() - start_time) * 1000
            print(f"âœ… [CACHE] æœç´¢ç»“æœæ•°æ®åº“æŸ¥è¯¢å®Œæˆï¼Œæ€»è€—æ—¶: {query_time:.2f}ms")

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            if hasattr(result, 'model_dump'):
                result_dict = result.model_dump()
            elif hasattr(result, '__dict__'):
                result_dict = result.__dict__
            else:
                result_dict = result

            # æ·»åŠ æ€§èƒ½ä¿¡æ¯
            result_dict.update({
                "_cache_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "_query_performance": {
                    "query_type": "search_results",
                    "keyword": keyword,
                    "keyword_hash": self._generate_keyword_hash(keyword),
                    "page": page,
                    "size": size,
                    "file_type": file_type,
                    "total_ms": round(query_time, 2)
                }
            })

            result_count = len(result_dict.get('documents', []))
            total_count = result_dict.get('total', 0)
            print(f"ğŸ—„ï¸ [CACHE] æœç´¢ç»“æœ: å½“å‰é¡µ{result_count}æ¡, æ€»è®¡{total_count}æ¡")
            return result_dict

        except Exception as e:
            query_time = (time.time() - start_time) * 1000
            print(f"âŒ [CACHE] æœç´¢ç»“æœæ•°æ®åº“æŸ¥è¯¢å¤±è´¥ ({query_time:.2f}ms): {e}")
            raise

    async def _get_from_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [CACHE] Redisä¸å¯ç”¨ï¼Œè·³è¿‡ç¼“å­˜è¯»å–")
            return None

        try:
            start_time = time.time()
            cached_str = self.redis_client.get(cache_key)
            read_time = (time.time() - start_time) * 1000

            if cached_str:
                data = json.loads(cached_str)
                print(f"ğŸ’¾ [CACHE] ç¼“å­˜è¯»å–æˆåŠŸ ({read_time:.2f}ms), æ•°æ®å¤§å°: {len(cached_str)} bytes")
                return data
            else:
                print(f"ğŸ’¾ [CACHE] ç¼“å­˜Keyä¸å­˜åœ¨ ({read_time:.2f}ms)")
                return None

        except Exception as e:
            print(f"âŒ [CACHE] ç¼“å­˜è¯»å–å¤±è´¥: {e}")
            return None

    async def _save_to_cache(self, cache_key: str, data: Dict[str, Any]) -> bool:
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [CACHE] Redisä¸å¯ç”¨ï¼Œè·³è¿‡ç¼“å­˜å†™å…¥")
            return False

        try:
            start_time = time.time()
            data_str = json.dumps(data, ensure_ascii=False, default=str)
            success = self.redis_client.setex(cache_key, self.search_ttl, data_str)
            write_time = (time.time() - start_time) * 1000

            if success:
                print(f"ğŸ’¾ [CACHE] ç¼“å­˜å†™å…¥æˆåŠŸ ({write_time:.2f}ms)")
                print(f"ğŸ’¾ [CACHE] æ•°æ®å¤§å°: {len(data_str)} bytes, TTL: {self.search_ttl}ç§’")
                return True
            else:
                print(f"âš ï¸ [CACHE] ç¼“å­˜å†™å…¥å¤±è´¥ ({write_time:.2f}ms)")
                return False

        except Exception as e:
            print(f"âŒ [CACHE] ç¼“å­˜å†™å…¥å¼‚å¸¸: {e}")
            return False

    async def invalidate_search_cache_by_keyword(self, keyword: str) -> bool:
        """æ¸…é™¤æŒ‡å®šå…³é”®è¯çš„æ‰€æœ‰æœç´¢ç¼“å­˜"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [CACHE] Redisä¸å¯ç”¨ï¼Œæ— æ³•æ¸…é™¤ç¼“å­˜")
            return False

        try:
            keyword_hash = self._generate_keyword_hash(keyword)
            pattern = f"{self.key_prefix}:keyword_{keyword_hash}:*"
            keys = self.redis_client.scan_iter(match=pattern)

            deleted_count = 0
            for key in keys:
                if self.redis_client.delete(key):
                    deleted_count += 1

            print(f"âœ… [CACHE] å…³é”®è¯'{keyword}'çš„æœç´¢ç¼“å­˜å·²æ¸…é™¤: {deleted_count}ä¸ªKey")
            return True

        except Exception as e:
            print(f"âŒ [CACHE] æ¸…é™¤å…³é”®è¯æœç´¢ç¼“å­˜å¤±è´¥: {e}")
            return False

    async def invalidate_all_search_cache(self) -> bool:
        """æ¸…é™¤æ‰€æœ‰æœç´¢ç¼“å­˜"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [CACHE] Redisä¸å¯ç”¨ï¼Œæ— æ³•æ¸…é™¤ç¼“å­˜")
            return False

        try:
            pattern = f"{self.key_prefix}:*"
            keys = self.redis_client.scan_iter(match=pattern)

            deleted_count = 0
            for key in keys:
                if self.redis_client.delete(key):
                    deleted_count += 1

            print(f"âœ… [CACHE] æ‰€æœ‰æœç´¢ç¼“å­˜å·²æ¸…é™¤: {deleted_count}ä¸ªKey")
            return True

        except Exception as e:
            print(f"âŒ [CACHE] æ¸…é™¤æ‰€æœ‰æœç´¢ç¼“å­˜å¤±è´¥: {e}")
            return False

    async def get_search_cache_stats(self) -> Dict[str, Any]:
        """è·å–æœç´¢ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {
                "total_search_keys": 0,
                "cache_size_bytes": 0,
                "unique_keywords": set(),
                "sample_keys": []
            }

            if not self.redis_client.is_available():
                stats["error"] = "Redisä¸å¯ç”¨"
                return stats

            # è·å–æ‰€æœ‰æœç´¢ç¼“å­˜Key
            pattern = f"{self.key_prefix}:*"
            keys = list(self.redis_client.scan_iter(match=pattern))

            stats["total_search_keys"] = len(keys)
            stats["sample_keys"] = keys[:5]  # å–å‰5ä¸ªä½œä¸ºæ ·æœ¬

            # ç»Ÿè®¡ç¼“å­˜å¤§å°å’Œå…³é”®è¯
            total_size = 0
            for key in keys:
                try:
                    # è·å–Keyçš„å¤§å°
                    value = self.redis_client.get(key)
                    if value:
                        total_size += len(value)

                    # æå–å…³é”®è¯å“ˆå¸Œ
                    if ":keyword_" in key:
                        keyword_hash = key.split(":keyword_")[1].split(":")[0]
                        stats["unique_keywords"].add(keyword_hash)

                except Exception:
                    continue

            stats["cache_size_bytes"] = total_size
            stats["unique_keywords"] = len(stats["unique_keywords"])

            print(f"ğŸ“Š [CACHE] æœç´¢ç¼“å­˜ç»Ÿè®¡: {stats}")
            return stats

        except Exception as e:
            print(f"âŒ [CACHE] è·å–æœç´¢ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {e}")
            return {"error": str(e)}


# å…¨å±€å®ä¾‹
search_cache_service = SearchCacheService()