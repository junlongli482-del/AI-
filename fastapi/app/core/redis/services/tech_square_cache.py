"""
æŠ€æœ¯å¹¿åœºç»Ÿè®¡ç¼“å­˜æœåŠ¡
åŠŸèƒ½ï¼šä¸“é—¨å¤„ç†æŠ€æœ¯å¹¿åœºç»Ÿè®¡æ•°æ®çš„ç¼“å­˜é€»è¾‘
"""
import json
import time
from typing import Dict, Any, Optional
from sqlalchemy.orm import Session
from sqlalchemy import func
from datetime import datetime

from ..client import RedisClient
from ....modules.v2.document_publish.models import PublishRecord
from ....modules.v2.document_manager.models import Document


class TechSquareStatsCacheService:
    """æŠ€æœ¯å¹¿åœºç»Ÿè®¡ç¼“å­˜æœåŠ¡"""

    def __init__(self):
        self.redis_client = RedisClient()

        # ç¼“å­˜é…ç½® - æŠ€æœ¯å¹¿åœºæ•°æ®å˜åŒ–æ›´é¢‘ç¹ï¼ŒTTLè®¾ç½®æ›´çŸ­
        self.ttl = 900  # 15åˆ†é’Ÿ
        self.key_prefix = "stats"
        self.cache_key = f"{self.key_prefix}:tech_square:global"

        print(f"ğŸ›ï¸ [TECH_SQUARE_CACHE] æŠ€æœ¯å¹¿åœºç»Ÿè®¡ç¼“å­˜æœåŠ¡åˆå§‹åŒ–")
        print(f"ğŸ›ï¸ [TECH_SQUARE_CACHE] TTL: {self.ttl}ç§’, Rediså¯ç”¨: {self.redis_client.is_available()}")
        print(f"ğŸ›ï¸ [TECH_SQUARE_CACHE] ç¼“å­˜Key: {self.cache_key}")

    async def get_tech_square_stats(self, db: Session) -> Dict[str, Any]:
        """
        è·å–æŠ€æœ¯å¹¿åœºç»Ÿè®¡ä¿¡æ¯ï¼ˆç¼“å­˜ä¼˜åŒ–ç‰ˆï¼‰

        ç»Ÿè®¡å†…å®¹ï¼š
        - æ€»å‘å¸ƒæ–‡æ¡£æ•°
        - æ€»æµè§ˆé‡
        - ä»Šæ—¥å‘å¸ƒæ•°
        - ç²¾é€‰æ–‡æ¡£æ•°
        - åˆ†ç±»ç»Ÿè®¡ï¼ˆMD/PDFï¼‰
        """
        print(f"ğŸ›ï¸ [TECH_SQUARE_CACHE] å¼€å§‹è·å–æŠ€æœ¯å¹¿åœºç»Ÿè®¡ç¼“å­˜...")
        print(f"ğŸ›ï¸ [TECH_SQUARE_CACHE] ç¼“å­˜Key: {self.cache_key}")

        # ğŸ” ç¬¬ä¸€æ­¥ï¼šå°è¯•ä»ç¼“å­˜è·å–
        cached_data = await self._get_from_cache()
        if cached_data:
            print(f"âœ… [TECH_SQUARE_CACHE] ç¼“å­˜å‘½ä¸­! è¿”å›ç¼“å­˜æ•°æ®")

            # æ·»åŠ ç¼“å­˜ä¿¡æ¯
            cached_data["cache_info"] = {
                "cached": True,
                "cache_time": cached_data.get("_cache_time"),
                "ttl_remaining": self.redis_client.ttl(self.cache_key),
                "cache_type": "tech_square_stats"
            }

            return cached_data

        # ğŸ—„ï¸ ç¬¬äºŒæ­¥ï¼šç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“
        print(f"âŒ [TECH_SQUARE_CACHE] ç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“...")
        stats_data = await self._query_database_stats(db)

        # ğŸ’¾ ç¬¬ä¸‰æ­¥ï¼šå†™å…¥ç¼“å­˜
        await self._save_to_cache(stats_data)

        # æ·»åŠ ç¼“å­˜ä¿¡æ¯
        stats_data["cache_info"] = {
            "cached": False,
            "cache_time": stats_data.get("_cache_time"),
            "ttl_remaining": self.ttl,
            "cache_type": "tech_square_stats"
        }

        return stats_data

    async def _query_database_stats(self, db: Session) -> Dict[str, Any]:
        """æŸ¥è¯¢æ•°æ®åº“ç»Ÿè®¡æ•°æ®ï¼ˆå¸¦è¯¦ç»†æ€§èƒ½ç›‘æ§ï¼‰"""
        print(f"ğŸ—„ï¸ [TECH_SQUARE_CACHE] å¼€å§‹æ•°æ®åº“æŸ¥è¯¢...")
        start_time = time.time()

        try:
            # æŸ¥è¯¢1ï¼šæ€»å‘å¸ƒæ–‡æ¡£æ•°
            query1_start = time.time()
            total_documents = db.query(PublishRecord).filter(
                PublishRecord.publish_status == 'published'
            ).count()
            query1_time = (time.time() - query1_start) * 1000
            print(f"ğŸ—„ï¸ [TECH_SQUARE_CACHE] æŸ¥è¯¢1å®Œæˆ: æ€»å‘å¸ƒæ–‡æ¡£æ•° = {total_documents} ({query1_time:.2f}ms)")

            # æŸ¥è¯¢2ï¼šæ€»æµè§ˆé‡
            query2_start = time.time()
            total_views = db.query(
                func.sum(PublishRecord.view_count)
            ).filter(
                PublishRecord.publish_status == 'published'
            ).scalar() or 0
            query2_time = (time.time() - query2_start) * 1000
            print(f"ğŸ—„ï¸ [TECH_SQUARE_CACHE] æŸ¥è¯¢2å®Œæˆ: æ€»æµè§ˆé‡ = {total_views} ({query2_time:.2f}ms)")

            # æŸ¥è¯¢3ï¼šä»Šæ—¥å‘å¸ƒæ•°
            query3_start = time.time()
            today_start = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)
            today_published = db.query(PublishRecord).filter(
                PublishRecord.publish_status == 'published',
                PublishRecord.publish_time >= today_start
            ).count()
            query3_time = (time.time() - query3_start) * 1000
            print(f"ğŸ—„ï¸ [TECH_SQUARE_CACHE] æŸ¥è¯¢3å®Œæˆ: ä»Šæ—¥å‘å¸ƒæ•° = {today_published} ({query3_time:.2f}ms)")

            # æŸ¥è¯¢4ï¼šç²¾é€‰æ–‡æ¡£æ•°
            query4_start = time.time()
            featured_count = db.query(PublishRecord).filter(
                PublishRecord.publish_status == 'published',
                PublishRecord.is_featured == True
            ).count()
            query4_time = (time.time() - query4_start) * 1000
            print(f"ğŸ—„ï¸ [TECH_SQUARE_CACHE] æŸ¥è¯¢4å®Œæˆ: ç²¾é€‰æ–‡æ¡£æ•° = {featured_count} ({query4_time:.2f}ms)")

            # æŸ¥è¯¢5ï¼šåˆ†ç±»ç»Ÿè®¡ï¼ˆMD/PDFï¼‰
            query5_start = time.time()
            category_stats = db.query(
                Document.file_type,
                func.count(Document.id)
            ).join(
                PublishRecord, Document.id == PublishRecord.document_id
            ).filter(
                PublishRecord.publish_status == 'published'
            ).group_by(Document.file_type).all()
            query5_time = (time.time() - query5_start) * 1000
            print(f"ğŸ—„ï¸ [TECH_SQUARE_CACHE] æŸ¥è¯¢5å®Œæˆ: åˆ†ç±»ç»Ÿè®¡ = {len(category_stats)}ç§ç±»å‹ ({query5_time:.2f}ms)")

            # æ ¼å¼åŒ–åˆ†ç±»ç»Ÿè®¡
            category_dict = {'md': 0, 'pdf': 0}
            for file_type, count in category_stats:
                if file_type in category_dict:
                    category_dict[file_type] = count

            # æ„å»ºç»“æœ
            result = {
                "total_documents": total_documents,
                "total_views": int(total_views),
                "today_published": today_published,
                "featured_count": featured_count,
                "category_stats": {
                    "md_count": category_dict['md'],
                    "pdf_count": category_dict['pdf'],
                    "total_count": category_dict['md'] + category_dict['pdf']
                },
                "_cache_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "_query_performance": {
                    "total_documents_ms": round(query1_time, 2),
                    "total_views_ms": round(query2_time, 2),
                    "today_published_ms": round(query3_time, 2),
                    "featured_count_ms": round(query4_time, 2),
                    "category_stats_ms": round(query5_time, 2),
                    "total_ms": round((time.time() - start_time) * 1000, 2)
                }
            }

            total_time = (time.time() - start_time) * 1000
            print(f"âœ… [TECH_SQUARE_CACHE] æ•°æ®åº“æŸ¥è¯¢å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ms")
            print(
                f"ğŸ“Š [TECH_SQUARE_CACHE] ç»Ÿè®¡ç»“æœ: æ–‡æ¡£{total_documents}ç¯‡, æµè§ˆ{total_views}æ¬¡, ä»Šæ—¥{today_published}ç¯‡, ç²¾é€‰{featured_count}ç¯‡")

            return result

        except Exception as e:
            query_time = (time.time() - start_time) * 1000
            print(f"âŒ [TECH_SQUARE_CACHE] æ•°æ®åº“æŸ¥è¯¢å¤±è´¥ ({query_time:.2f}ms): {e}")
            raise

    async def _get_from_cache(self) -> Optional[Dict[str, Any]]:
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [TECH_SQUARE_CACHE] Redisä¸å¯ç”¨ï¼Œè·³è¿‡ç¼“å­˜è¯»å–")
            return None

        try:
            start_time = time.time()
            cached_str = self.redis_client.get(self.cache_key)
            read_time = (time.time() - start_time) * 1000

            if cached_str:
                data = json.loads(cached_str)
                print(f"ğŸ’¾ [TECH_SQUARE_CACHE] ç¼“å­˜è¯»å–æˆåŠŸ ({read_time:.2f}ms), æ•°æ®å¤§å°: {len(cached_str)} bytes")
                return data
            else:
                print(f"ğŸ’¾ [TECH_SQUARE_CACHE] ç¼“å­˜Keyä¸å­˜åœ¨ ({read_time:.2f}ms)")
                return None

        except Exception as e:
            print(f"âŒ [TECH_SQUARE_CACHE] ç¼“å­˜è¯»å–å¤±è´¥: {e}")
            return None

    async def _save_to_cache(self, data: Dict[str, Any]) -> bool:
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [TECH_SQUARE_CACHE] Redisä¸å¯ç”¨ï¼Œè·³è¿‡ç¼“å­˜å†™å…¥")
            return False

        try:
            start_time = time.time()
            data_str = json.dumps(data, ensure_ascii=False)
            success = self.redis_client.setex(self.cache_key, self.ttl, data_str)
            write_time = (time.time() - start_time) * 1000

            if success:
                print(f"ğŸ’¾ [TECH_SQUARE_CACHE] ç¼“å­˜å†™å…¥æˆåŠŸ ({write_time:.2f}ms)")
                print(f"ğŸ’¾ [TECH_SQUARE_CACHE] æ•°æ®å¤§å°: {len(data_str)} bytes, TTL: {self.ttl}ç§’")
                return True
            else:
                print(f"âš ï¸ [TECH_SQUARE_CACHE] ç¼“å­˜å†™å…¥å¤±è´¥ ({write_time:.2f}ms)")
                return False

        except Exception as e:
            print(f"âŒ [TECH_SQUARE_CACHE] ç¼“å­˜å†™å…¥å¼‚å¸¸: {e}")
            return False

    async def invalidate_cache(self) -> bool:
        """æ¸…é™¤æŠ€æœ¯å¹¿åœºç»Ÿè®¡ç¼“å­˜ï¼ˆå½“æœ‰æ–‡æ¡£å‘å¸ƒ/åˆ é™¤æ—¶è°ƒç”¨ï¼‰"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [TECH_SQUARE_CACHE] Redisä¸å¯ç”¨ï¼Œæ— æ³•æ¸…é™¤ç¼“å­˜")
            return False

        try:
            result = self.redis_client.delete(self.cache_key)
            if result:
                print(f"âœ… [TECH_SQUARE_CACHE] æŠ€æœ¯å¹¿åœºç»Ÿè®¡ç¼“å­˜å·²æ¸…é™¤: {self.cache_key}")
            else:
                print(f"â„¹ï¸ [TECH_SQUARE_CACHE] ç¼“å­˜Keyä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…é™¤: {self.cache_key}")
            return bool(result)

        except Exception as e:
            print(f"âŒ [TECH_SQUARE_CACHE] æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")
            return False


# å…¨å±€å®ä¾‹
tech_square_stats_cache_service = TechSquareStatsCacheService()