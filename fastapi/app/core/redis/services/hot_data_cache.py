"""
çƒ­é—¨æ•°æ®ç¼“å­˜æœåŠ¡
åŠŸèƒ½ï¼šä¸“é—¨å¤„ç†çƒ­é—¨æ–‡æ¡£å’Œæœ€æ–°æ–‡æ¡£çš„ç¼“å­˜é€»è¾‘
"""
import json
import time
from typing import Dict, Any, Optional, Callable
from sqlalchemy.orm import Session

from ..client import RedisClient


class HotDataCacheService:
    """çƒ­é—¨æ•°æ®ç¼“å­˜æœåŠ¡"""

    def __init__(self):
        self.redis_client = RedisClient()

        # ç¼“å­˜é…ç½®
        self.hot_docs_ttl = 600  # çƒ­é—¨æ–‡æ¡£ç¼“å­˜10åˆ†é’Ÿ
        self.latest_docs_ttl = 300  # æœ€æ–°æ–‡æ¡£ç¼“å­˜5åˆ†é’Ÿ
        self.key_prefix = "hot_data"

        print(f"ğŸ”¥ [CACHE] çƒ­é—¨æ•°æ®ç¼“å­˜æœåŠ¡åˆå§‹åŒ–")
        print(f"ğŸ”¥ [CACHE] çƒ­é—¨æ–‡æ¡£TTL: {self.hot_docs_ttl}ç§’, æœ€æ–°æ–‡æ¡£TTL: {self.latest_docs_ttl}ç§’")
        print(f"ğŸ”¥ [CACHE] Rediså¯ç”¨: {self.redis_client.is_available()}")

    def _build_hot_docs_cache_key(self, limit: int) -> str:
        """æ„å»ºçƒ­é—¨æ–‡æ¡£ç¼“å­˜Key"""
        key = f"{self.key_prefix}:hot_docs:limit_{limit}"
        print(f"ğŸ”‘ [CACHE] æ„å»ºçƒ­é—¨æ–‡æ¡£ç¼“å­˜Key: {key}")
        return key

    def _build_latest_docs_cache_key(self, limit: int) -> str:
        """æ„å»ºæœ€æ–°æ–‡æ¡£ç¼“å­˜Key"""
        key = f"{self.key_prefix}:latest_docs:limit_{limit}"
        print(f"ğŸ”‘ [CACHE] æ„å»ºæœ€æ–°æ–‡æ¡£ç¼“å­˜Key: {key}")
        return key

    async def get_hot_documents(self, db: Session, query_func: Callable, limit: int = 10) -> Dict[str, Any]:
        """
        è·å–çƒ­é—¨æ–‡æ¡£åˆ—è¡¨ï¼ˆç¼“å­˜ä¼˜åŒ–ç‰ˆï¼‰
        """
        cache_key = self._build_hot_docs_cache_key(limit)

        print(f"ğŸ”¥ [CACHE] å¼€å§‹è·å–çƒ­é—¨æ–‡æ¡£ç¼“å­˜...")
        print(f"ğŸ”¥ [CACHE] é™åˆ¶æ•°é‡: {limit}, ç¼“å­˜Key: {cache_key}")

        # ğŸ” ç¬¬ä¸€æ­¥ï¼šå°è¯•ä»ç¼“å­˜è·å–
        cached_data = await self._get_from_cache(cache_key)
        if cached_data:
            print(f"âœ… [CACHE] çƒ­é—¨æ–‡æ¡£ç¼“å­˜å‘½ä¸­! è¿”å›ç¼“å­˜æ•°æ®")

            # æ·»åŠ ç¼“å­˜ä¿¡æ¯
            cached_data["cache_info"] = {
                "cached": True,
                "cache_time": cached_data.get("_cache_time"),
                "ttl_remaining": self.redis_client.ttl(cache_key)
            }

            return cached_data

        # ğŸ—„ï¸ ç¬¬äºŒæ­¥ï¼šç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“
        print(f"âŒ [CACHE] çƒ­é—¨æ–‡æ¡£ç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“...")
        docs_data = await self._query_hot_documents(db, query_func, limit)

        # ğŸ’¾ ç¬¬ä¸‰æ­¥ï¼šå†™å…¥ç¼“å­˜
        await self._save_to_cache(cache_key, docs_data, self.hot_docs_ttl)

        # æ·»åŠ ç¼“å­˜ä¿¡æ¯
        docs_data["cache_info"] = {
            "cached": False,
            "cache_time": docs_data.get("_cache_time"),
            "ttl_remaining": self.hot_docs_ttl
        }

        return docs_data

    async def get_latest_documents(self, db: Session, query_func: Callable, limit: int = 10) -> Dict[str, Any]:
        """
        è·å–æœ€æ–°æ–‡æ¡£åˆ—è¡¨ï¼ˆç¼“å­˜ä¼˜åŒ–ç‰ˆï¼‰
        """
        cache_key = self._build_latest_docs_cache_key(limit)

        print(f"ğŸ“… [CACHE] å¼€å§‹è·å–æœ€æ–°æ–‡æ¡£ç¼“å­˜...")
        print(f"ğŸ“… [CACHE] é™åˆ¶æ•°é‡: {limit}, ç¼“å­˜Key: {cache_key}")

        # ğŸ” ç¬¬ä¸€æ­¥ï¼šå°è¯•ä»ç¼“å­˜è·å–
        cached_data = await self._get_from_cache(cache_key)
        if cached_data:
            print(f"âœ… [CACHE] æœ€æ–°æ–‡æ¡£ç¼“å­˜å‘½ä¸­! è¿”å›ç¼“å­˜æ•°æ®")

            # æ·»åŠ ç¼“å­˜ä¿¡æ¯
            cached_data["cache_info"] = {
                "cached": True,
                "cache_time": cached_data.get("_cache_time"),
                "ttl_remaining": self.redis_client.ttl(cache_key)
            }

            return cached_data

        # ğŸ—„ï¸ ç¬¬äºŒæ­¥ï¼šç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“
        print(f"âŒ [CACHE] æœ€æ–°æ–‡æ¡£ç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“...")
        docs_data = await self._query_latest_documents(db, query_func, limit)

        # ğŸ’¾ ç¬¬ä¸‰æ­¥ï¼šå†™å…¥ç¼“å­˜
        await self._save_to_cache(cache_key, docs_data, self.latest_docs_ttl)

        # æ·»åŠ ç¼“å­˜ä¿¡æ¯
        docs_data["cache_info"] = {
            "cached": False,
            "cache_time": docs_data.get("_cache_time"),
            "ttl_remaining": self.latest_docs_ttl
        }

        return docs_data

    async def _query_hot_documents(self, db: Session, query_func: Callable, limit: int) -> Dict[str, Any]:
        """æŸ¥è¯¢çƒ­é—¨æ–‡æ¡£æ•°æ®ï¼ˆå¸¦æ€§èƒ½ç›‘æ§ï¼‰"""
        print(f"ğŸ—„ï¸ [CACHE] å¼€å§‹æŸ¥è¯¢çƒ­é—¨æ–‡æ¡£æ•°æ®åº“...")
        start_time = time.time()

        try:
            # è°ƒç”¨ä¼ å…¥çš„æŸ¥è¯¢å‡½æ•°
            result = query_func(limit=limit)

            query_time = (time.time() - start_time) * 1000
            print(f"âœ… [CACHE] çƒ­é—¨æ–‡æ¡£æ•°æ®åº“æŸ¥è¯¢å®Œæˆï¼Œæ€»è€—æ—¶: {query_time:.2f}ms")

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            if hasattr(result, 'model_dump'):
                result_dict = result.model_dump()
            elif hasattr(result, '__dict__'):
                result_dict = result.__dict__
            else:
                result_dict = result

            # æ·»åŠ æ€§èƒ½ä¿¡æ¯
            result_dict.update({
                "_cache_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "_query_performance": {
                    "query_type": "hot_documents",
                    "limit": limit,
                    "total_ms": round(query_time, 2)
                }
            })

            print(f"ğŸ—„ï¸ [CACHE] çƒ­é—¨æ–‡æ¡£æ•°é‡: {len(result_dict.get('documents', []))}")
            return result_dict

        except Exception as e:
            query_time = (time.time() - start_time) * 1000
            print(f"âŒ [CACHE] çƒ­é—¨æ–‡æ¡£æ•°æ®åº“æŸ¥è¯¢å¤±è´¥ ({query_time:.2f}ms): {e}")
            raise

    async def _query_latest_documents(self, db: Session, query_func: Callable, limit: int) -> Dict[str, Any]:
        """æŸ¥è¯¢æœ€æ–°æ–‡æ¡£æ•°æ®ï¼ˆå¸¦æ€§èƒ½ç›‘æ§ï¼‰"""
        print(f"ğŸ—„ï¸ [CACHE] å¼€å§‹æŸ¥è¯¢æœ€æ–°æ–‡æ¡£æ•°æ®åº“...")
        start_time = time.time()

        try:
            # è°ƒç”¨ä¼ å…¥çš„æŸ¥è¯¢å‡½æ•°
            result = query_func(limit=limit)

            query_time = (time.time() - start_time) * 1000
            print(f"âœ… [CACHE] æœ€æ–°æ–‡æ¡£æ•°æ®åº“æŸ¥è¯¢å®Œæˆï¼Œæ€»è€—æ—¶: {query_time:.2f}ms")

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            if hasattr(result, 'model_dump'):
                result_dict = result.model_dump()
            elif hasattr(result, '__dict__'):
                result_dict = result.__dict__
            else:
                result_dict = result

            # æ·»åŠ æ€§èƒ½ä¿¡æ¯
            result_dict.update({
                "_cache_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "_query_performance": {
                    "query_type": "latest_documents",
                    "limit": limit,
                    "total_ms": round(query_time, 2)
                }
            })

            print(f"ğŸ—„ï¸ [CACHE] æœ€æ–°æ–‡æ¡£æ•°é‡: {len(result_dict.get('documents', []))}")
            return result_dict

        except Exception as e:
            query_time = (time.time() - start_time) * 1000
            print(f"âŒ [CACHE] æœ€æ–°æ–‡æ¡£æ•°æ®åº“æŸ¥è¯¢å¤±è´¥ ({query_time:.2f}ms): {e}")
            raise

    async def _get_from_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [CACHE] Redisä¸å¯ç”¨ï¼Œè·³è¿‡ç¼“å­˜è¯»å–")
            return None

        try:
            start_time = time.time()
            cached_str = self.redis_client.get(cache_key)
            read_time = (time.time() - start_time) * 1000

            if cached_str:
                data = json.loads(cached_str)
                print(f"ğŸ’¾ [CACHE] ç¼“å­˜è¯»å–æˆåŠŸ ({read_time:.2f}ms), æ•°æ®å¤§å°: {len(cached_str)} bytes")
                return data
            else:
                print(f"ğŸ’¾ [CACHE] ç¼“å­˜Keyä¸å­˜åœ¨ ({read_time:.2f}ms)")
                return None

        except Exception as e:
            print(f"âŒ [CACHE] ç¼“å­˜è¯»å–å¤±è´¥: {e}")
            return None

    async def _save_to_cache(self, cache_key: str, data: Dict[str, Any], ttl: int) -> bool:
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [CACHE] Redisä¸å¯ç”¨ï¼Œè·³è¿‡ç¼“å­˜å†™å…¥")
            return False

        try:
            start_time = time.time()
            data_str = json.dumps(data, ensure_ascii=False, default=str)
            success = self.redis_client.setex(cache_key, ttl, data_str)
            write_time = (time.time() - start_time) * 1000

            if success:
                print(f"ğŸ’¾ [CACHE] ç¼“å­˜å†™å…¥æˆåŠŸ ({write_time:.2f}ms)")
                print(f"ğŸ’¾ [CACHE] æ•°æ®å¤§å°: {len(data_str)} bytes, TTL: {ttl}ç§’")
                return True
            else:
                print(f"âš ï¸ [CACHE] ç¼“å­˜å†™å…¥å¤±è´¥ ({write_time:.2f}ms)")
                return False

        except Exception as e:
            print(f"âŒ [CACHE] ç¼“å­˜å†™å…¥å¼‚å¸¸: {e}")
            return False

    async def invalidate_hot_documents_cache(self) -> bool:
        """æ¸…é™¤æ‰€æœ‰çƒ­é—¨æ–‡æ¡£ç¼“å­˜"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [CACHE] Redisä¸å¯ç”¨ï¼Œæ— æ³•æ¸…é™¤ç¼“å­˜")
            return False

        try:
            # è·å–æ‰€æœ‰çƒ­é—¨æ–‡æ¡£ç¼“å­˜Key
            pattern = f"{self.key_prefix}:hot_docs:*"
            keys = self.redis_client.scan_iter(match=pattern)

            deleted_count = 0
            for key in keys:
                if self.redis_client.delete(key):
                    deleted_count += 1

            print(f"âœ… [CACHE] çƒ­é—¨æ–‡æ¡£ç¼“å­˜å·²æ¸…é™¤: {deleted_count}ä¸ªKey")
            return True

        except Exception as e:
            print(f"âŒ [CACHE] æ¸…é™¤çƒ­é—¨æ–‡æ¡£ç¼“å­˜å¤±è´¥: {e}")
            return False

    async def invalidate_latest_documents_cache(self) -> bool:
        """æ¸…é™¤æ‰€æœ‰æœ€æ–°æ–‡æ¡£ç¼“å­˜"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [CACHE] Redisä¸å¯ç”¨ï¼Œæ— æ³•æ¸…é™¤ç¼“å­˜")
            return False

        try:
            # è·å–æ‰€æœ‰æœ€æ–°æ–‡æ¡£ç¼“å­˜Key
            pattern = f"{self.key_prefix}:latest_docs:*"
            keys = self.redis_client.scan_iter(match=pattern)

            deleted_count = 0
            for key in keys:
                if self.redis_client.delete(key):
                    deleted_count += 1

            print(f"âœ… [CACHE] æœ€æ–°æ–‡æ¡£ç¼“å­˜å·²æ¸…é™¤: {deleted_count}ä¸ªKey")
            return True

        except Exception as e:
            print(f"âŒ [CACHE] æ¸…é™¤æœ€æ–°æ–‡æ¡£ç¼“å­˜å¤±è´¥: {e}")
            return False

    async def invalidate_all_hot_data_cache(self) -> bool:
        """æ¸…é™¤æ‰€æœ‰çƒ­é—¨æ•°æ®ç¼“å­˜"""
        if not self.redis_client.is_available():
            print(f"âš ï¸ [CACHE] Redisä¸å¯ç”¨ï¼Œæ— æ³•æ¸…é™¤ç¼“å­˜")
            return False

        try:
            # è·å–æ‰€æœ‰çƒ­é—¨æ•°æ®ç¼“å­˜Key
            pattern = f"{self.key_prefix}:*"
            keys = self.redis_client.scan_iter(match=pattern)

            deleted_count = 0
            for key in keys:
                if self.redis_client.delete(key):
                    deleted_count += 1

            print(f"âœ… [CACHE] æ‰€æœ‰çƒ­é—¨æ•°æ®ç¼“å­˜å·²æ¸…é™¤: {deleted_count}ä¸ªKey")
            return True

        except Exception as e:
            print(f"âŒ [CACHE] æ¸…é™¤æ‰€æœ‰çƒ­é—¨æ•°æ®ç¼“å­˜å¤±è´¥: {e}")
            return False


# å…¨å±€å®ä¾‹
hot_data_cache_service = HotDataCacheService()